{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Time Series Analysis\n",
    "## Using PyTorch, fastai, and tsai\n",
    "\n",
    "This notebook provides a practical introduction to modern deep learning techniques for time series analysis.\n",
    "\n",
    "### What You'll Learn:\n",
    "1. Setting up the environment\n",
    "2. Data preparation for time series\n",
    "3. Building models from scratch with PyTorch\n",
    "4. Using tsai for high-level APIs\n",
    "5. Different architectures (LSTM, CNN, Transformers)\n",
    "6. Evaluation and forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation and Setup\n",
    "\n",
    "First, let's install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment to install required packages\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install fastai\n",
    "# !pip install tsai\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Time Series Data\n",
    "\n",
    "We'll start with synthetic data to understand the concepts clearly. Later, you can apply these techniques to real datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_timeseries(n_samples=1000, n_features=1):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series with trend, seasonality, and noise\n",
    "    \"\"\"\n",
    "    time = np.arange(n_samples)\n",
    "    \n",
    "    # Trend component\n",
    "    trend = 0.02 * time\n",
    "    \n",
    "    # Seasonal component (yearly pattern)\n",
    "    seasonal = 10 * np.sin(2 * np.pi * time / 50)\n",
    "    \n",
    "    # Random noise\n",
    "    noise = np.random.normal(0, 2, n_samples)\n",
    "    \n",
    "    # Combine components\n",
    "    data = trend + seasonal + noise + 50  # Add baseline\n",
    "    \n",
    "    return time, data\n",
    "\n",
    "# Generate data\n",
    "time, data = generate_synthetic_timeseries(n_samples=1000)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(time, data, alpha=0.7)\n",
    "plt.title('Synthetic Time Series Data')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data range: [{data.min():.2f}, {data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Dataset Preparation\n",
    "\n",
    "Critical concept: We need to create sliding windows of past observations to predict future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for time series forecasting\n",
    "    \n",
    "    Args:\n",
    "        data: 1D array of time series values\n",
    "        window_size: Number of past time steps to use as input\n",
    "        forecast_horizon: Number of future time steps to predict\n",
    "    \"\"\"\n",
    "    def __init__(self, data, window_size=50, forecast_horizon=1):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.window_size = window_size\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size - self.forecast_horizon + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: window_size past observations\n",
    "        x = self.data[idx:idx + self.window_size]\n",
    "        # Target: next forecast_horizon observations\n",
    "        y = self.data[idx + self.window_size:idx + self.window_size + self.forecast_horizon]\n",
    "        \n",
    "        return x.unsqueeze(-1), y  # Add feature dimension to x\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split into train/validation/test (70/15/15)\n",
    "train_size = int(0.7 * len(data_normalized))\n",
    "val_size = int(0.15 * len(data_normalized))\n",
    "\n",
    "train_data = data_normalized[:train_size]\n",
    "val_data = data_normalized[train_size:train_size + val_size]\n",
    "test_data = data_normalized[train_size + val_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "window_size = 50\n",
    "forecast_horizon = 10  # Predict next 10 steps\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_data, window_size, forecast_horizon)\n",
    "val_dataset = TimeSeriesDataset(val_data, window_size, forecast_horizon)\n",
    "test_dataset = TimeSeriesDataset(test_data, window_size, forecast_horizon)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check a sample\n",
    "x_sample, y_sample = train_dataset[0]\n",
    "print(f\"\\nInput shape: {x_sample.shape} (window_size, features)\")\n",
    "print(f\"Target shape: {y_sample.shape} (forecast_horizon,)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture #1: LSTM (Baseline)\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks are a classic choice for time series. They can capture long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, \n",
    "                 forecast_horizon=10, dropout=0.2):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, forecast_horizon)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Predict future values\n",
    "        output = self.fc(last_hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Instantiate model\n",
    "lstm_model = LSTMForecaster(\n",
    "    input_size=1,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(lstm_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Standard PyTorch training loop with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x_batch, y_batch in loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(x_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training configuration\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                         factor=0.5, patience=5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(lstm_model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = validate(lstm_model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(lstm_model.state_dict(), '/home/claude/best_lstm_model.pth')\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "plt.plot(val_losses, label='Validation Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training History - LSTM')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture #2: CNN for Time Series\n",
    "\n",
    "CNNs can be surprisingly effective for time series by capturing local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, forecast_horizon=10, dropout=0.2):\n",
    "        super(CNNForecaster, self).__init__()\n",
    "        \n",
    "        # CNN layers with increasing dilation for multi-scale patterns\n",
    "        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=4, dilation=4)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, forecast_horizon)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        # Conv1d expects (batch, features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        \n",
    "        # Prediction\n",
    "        output = self.fc(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Instantiate and train CNN model\n",
    "cnn_model = CNNForecaster(\n",
    "    input_size=1,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(cnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using tsai Library (High-Level API)\n",
    "\n",
    "Now let's use tsai for a more streamlined workflow similar to fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if tsai is installed\n",
    "# from tsai.all import *\n",
    "\n",
    "# Example of using tsai (pseudo-code - requires tsai installation)\n",
    "\"\"\"\n",
    "# Prepare data in tsai format\n",
    "X = np.array([train_data[i:i+window_size] for i in range(len(train_data)-window_size-forecast_horizon)])\n",
    "y = np.array([train_data[i+window_size:i+window_size+forecast_horizon] for i in range(len(train_data)-window_size-forecast_horizon)])\n",
    "\n",
    "# Create splits\n",
    "splits = get_splits(y, valid_size=0.2, test_size=0.2, show_plot=False)\n",
    "\n",
    "# Create dataloaders\n",
    "tfms = [None, TSRegression()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=32)\n",
    "\n",
    "# Create model (InceptionTime is state-of-the-art)\n",
    "model = InceptionTime(dls.vars, dls.c, dls.len)\n",
    "\n",
    "# Train with fastai Learner\n",
    "learn = Learner(dls, model, loss_func=MSELossFlat(), metrics=[mae])\n",
    "learn.fit_one_cycle(50, 1e-3)\n",
    "\n",
    "# Make predictions\n",
    "preds, targets = learn.get_preds()\n",
    "\"\"\"\n",
    "\n",
    "print(\"\"\"To use tsai:\n",
    "1. Install: pip install tsai\n",
    "2. Import: from tsai.all import *\n",
    "3. Use high-level APIs similar to fastai\n",
    "4. Access state-of-the-art models: InceptionTime, XceptionTime, TST (Transformer)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Visualization\n",
    "\n",
    "Let's evaluate our models and visualize predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model and return predictions and targets\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            predictions = model(x_batch)\n",
    "            \n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_targets.append(y_batch.numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_preds, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((predictions - targets) ** 2)\n",
    "    mae = np.mean(np.abs(predictions - targets))\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    return predictions, targets, {'MSE': mse, 'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "# Evaluate LSTM\n",
    "lstm_model.load_state_dict(torch.load('/home/claude/best_lstm_model.pth'))\n",
    "lstm_preds, targets, lstm_metrics = evaluate_model(lstm_model, test_loader, device)\n",
    "\n",
    "print(\"LSTM Model Performance:\")\n",
    "for metric, value in lstm_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "n_samples_to_plot = 5\n",
    "fig, axes = plt.subplots(n_samples_to_plot, 1, figsize=(15, 10))\n",
    "\n",
    "for i in range(n_samples_to_plot):\n",
    "    axes[i].plot(targets[i], label='Actual', marker='o', alpha=0.7)\n",
    "    axes[i].plot(lstm_preds[i], label='LSTM Prediction', marker='s', alpha=0.7)\n",
    "    axes[i].set_ylabel('Value')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_title(f'Sample {i+1}')\n",
    "\n",
    "axes[-1].set_xlabel('Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Transformer for Time Series\n",
    "\n",
    "Transformers have shown excellent results for time series, especially for long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerForecaster(nn.Module):\n",
    "    def __init__(self, input_size=1, d_model=64, nhead=4, num_layers=2, \n",
    "                 forecast_horizon=10, dropout=0.1):\n",
    "        super(TransformerForecaster, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(d_model, forecast_horizon)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Use last timestep for prediction\n",
    "        x = x[:, -1, :]\n",
    "        output = self.fc(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Instantiate Transformer model\n",
    "transformer_model = TransformerForecaster(\n",
    "    input_size=1,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(transformer_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in transformer_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Real-World Applications\n",
    "\n",
    "### Try These Real Datasets:\n",
    "1. **Finance**: Stock prices, cryptocurrency data\n",
    "2. **Weather**: Temperature, precipitation forecasting\n",
    "3. **Energy**: Electricity consumption, solar generation\n",
    "4. **Retail**: Sales forecasting, demand prediction\n",
    "\n",
    "### Advanced Techniques to Explore:\n",
    "- **Multi-variate forecasting**: Multiple related time series\n",
    "- **Attention mechanisms**: Visualize what the model focuses on\n",
    "- **Probabilistic forecasting**: Predict distributions instead of point estimates\n",
    "- **Transfer learning**: Pre-train on one dataset, fine-tune on another\n",
    "- **Ensemble methods**: Combine multiple models\n",
    "\n",
    "### Recommended Libraries:\n",
    "- **tsai**: High-level API built on fastai\n",
    "- **PyTorch Forecasting**: Specialized for forecasting tasks\n",
    "- **Darts**: User-friendly forecasting library\n",
    "- **GluonTS**: Probabilistic time series models\n",
    "\n",
    "### Resources:\n",
    "- tsai documentation: https://timeseriesai.github.io/tsai/\n",
    "- PyTorch Forecasting: https://pytorch-forecasting.readthedocs.io/\n",
    "- Papers: \"Temporal Fusion Transformers\", \"N-BEATS\", \"Informer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise: Your Turn!\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Experiment with hyperparameters**: Change window_size, hidden_size, number of layers\n",
    "2. **Load real data**: Use pandas to load a CSV file with time series data\n",
    "3. **Multi-step ahead prediction**: Modify the model to predict multiple steps iteratively\n",
    "4. **Add external features**: Include additional variables (e.g., day of week, holidays)\n",
    "5. **Compare architectures**: Train CNN and Transformer models, compare performance\n",
    "6. **Implement attention visualization**: See what patterns the model learns\n",
    "7. **Try different loss functions**: MAE, Huber loss, quantile loss for probabilistic forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here!\n",
    "# Try modifying the code above or implementing new features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}